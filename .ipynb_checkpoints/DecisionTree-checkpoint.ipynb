{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eefa7771-6f65-48fe-b68a-fa17217c9456",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import log2\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def accuracy(pred, labels) -> float: #custom accuracy function\n",
    "    correct, total = 0, len(pred)\n",
    "    for i in range(len(pred)):\n",
    "        if labels.iloc[i] == pred[i]:\n",
    "            correct += 1\n",
    "    return correct/total\n",
    "\n",
    "def precision(pred, labels) -> float: #custom precision function\n",
    "    if pred.sum() == 0:\n",
    "        return 0.0 \n",
    "    \n",
    "    mask = pred == 1\n",
    "    return labels[mask].sum() / pred.sum()\n",
    "\n",
    "def recall(pred, labels) -> float: #custom recall function\n",
    "    if pred.sum() == len(pred):\n",
    "        return 0.0 \n",
    "    \n",
    "    mask = pred == 1\n",
    "    return labels[mask].sum() / labels.sum()\n",
    "\n",
    "appleData = pd.read_csv('apple_quality.csv') #predict if apple has good or bad quality\n",
    "appleData = appleData.iloc[:1000] #have to take a sample because my decision tree is too slow :(\n",
    "data = appleData.iloc[:, 1:appleData.shape[1]-1]\n",
    "labels = appleData['Quality'].replace({'good': 1, 'bad':0}) #encode labels\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cabc5ce7-599d-42ab-bf60-d503e067eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proportion of positive predictions = p1\n",
    "# proportion of negative predictions = p0\n",
    "# entropy = -p1 * log2(p1) - p0 * log2(p0)\n",
    "\n",
    "def entropy(labels: pd.Series) -> float:\n",
    "    p1 = (labels == 1).sum() / len(labels)\n",
    "    p0 = 1 - p1\n",
    "    \n",
    "    # log2(0) defined as 0\n",
    "    if p1 == 0 or p0 == 0:\n",
    "        return 0\n",
    "    \n",
    "    return -p1 * log2(p1) - p0 * log2(p0)\n",
    "\n",
    "# proportion of positive predictions = p1\n",
    "# proportion of negative predictions = p0\n",
    "# gini impurity = 1 - p1^2 - p0^2\n",
    "\n",
    "def giniImpurity(labels: pd.Series) -> float:\n",
    "    if len(labels) == 0:\n",
    "        return 0\n",
    "    p1 = (labels == 1).sum() / len(labels)\n",
    "    p0 = 1 - p1\n",
    "    p1 = p1 ** 2\n",
    "    p0 = p0 ** 2\n",
    "    return 1 - p1 - p0\n",
    "\n",
    "# proportion of labels less than split = pl\n",
    "# proportion of labels greater than split = pr\n",
    "# information gain = total - pl * instances < split - pr * instances > split\n",
    "\n",
    "def infoGain(data: pd.DataFrame, labels: pd.Series, col: str, split: float, criterion: str) -> float:\n",
    "    mask = data[col] <= split \n",
    "    left = labels[mask] # labels with corresponding data values <= split\n",
    "    right = labels[~mask] # labels with corresponding data values > split\n",
    "    \n",
    "    pl = np.mean(mask)\n",
    "    pr = 1 - pl\n",
    "    \n",
    "    if criterion == 'entropy':\n",
    "        return entropy(labels) - pl * entropy(left) - pr * entropy(right)\n",
    "    elif criterion == 'gini':\n",
    "        return giniImpurity(labels) - pl * giniImpurity(left) - pr * giniImpurity(right)\n",
    "\n",
    "#finds the best split in the current set of data\n",
    "\n",
    "def bestSplit(data: pd.DataFrame, labels: pd.Series, criterion: str) -> (str, float):\n",
    "    colName, splitVal, maxInfoGain = '', 0, -1\n",
    "    \n",
    "    for col in data.columns:\n",
    "        # sorts the data values of a column and finds the splits\n",
    "        uniqueVals = np.sort(data[col].unique())\n",
    "        splits = (uniqueVals[:-1] + uniqueVals[1:]) / 2\n",
    "        \n",
    "        for split in splits:\n",
    "            gain = infoGain(data, labels, col, split, criterion) #find the split that has the greatest information gain\n",
    "            if gain > maxInfoGain:\n",
    "                maxInfoGain = gain\n",
    "                colName = col\n",
    "                splitVal = split\n",
    "            \n",
    "    return (colName, splitVal)\n",
    "\n",
    "# recursively build decision tree\n",
    "def buildTree(data: pd.DataFrame, labels: pd.Series, criterion: str, min_split: int):\n",
    "    # at most min_split - 1 labels in a leaf node\n",
    "    if len(labels.unique()) < min_split:\n",
    "        return {'isLeaf': True,\n",
    "                'pred': labels.mode().iloc[0]} #finds the mode of the leaf node\n",
    "\n",
    "    (col, split) = bestSplit(data, labels, criterion)\n",
    "    leftData = data[data[col] <= split]\n",
    "    leftLabels = labels[data[col] <= split]\n",
    "    \n",
    "    rightData = data[data[col] > split]\n",
    "    rightLabels = labels[data[col] > split]\n",
    "\n",
    "    left = buildTree(leftData, leftLabels, criterion, min_split)\n",
    "    right = buildTree(rightData, rightLabels, criterion, min_split)\n",
    "\n",
    "    return {'column': col,\n",
    "            'split': split,\n",
    "            'left': left,\n",
    "            'right': right, \n",
    "            'isLeaf': False}\n",
    "\n",
    "tree = buildTree(xtrain, ytrain, 'entropy', 2)\n",
    "tree2 = buildTree(xtrain, ytrain, 'gini', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a265f22b-cd87-4260-a164-703f8463a3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Information Gain: \n",
      "accuracy: 0.81\n",
      "precision: 0.8282828282828283\n",
      "recall: 0.7961165048543689\n",
      "\n",
      "With Gini Impurity: \n",
      "accuracy: 0.82\n",
      "precision: 0.819047619047619\n",
      "recall: 0.8349514563106796\n"
     ]
    }
   ],
   "source": [
    "def predict(tree, data: pd.DataFrame):\n",
    "    if tree['isLeaf']:\n",
    "        return pd.Series([tree['pred']] * len(data), index=data.index) \n",
    "    \n",
    "    col = tree['column']\n",
    "    split = tree['split']\n",
    "    \n",
    "    left_data = data[data[col] <= split]\n",
    "    right_data = data[data[col] > split]\n",
    "    \n",
    "    pred = pd.Series(index=data.index)\n",
    "\n",
    "    pred[data[col] <= split] = predict(tree['left'], left_data)\n",
    "    pred[data[col] > split] = predict(tree['right'], right_data)\n",
    "\n",
    "    return pred\n",
    "\n",
    "print(\"With Information Gain: \")\n",
    "pred = np.array(predict(tree, xtest))\n",
    "\n",
    "acc = accuracy(pred, ytest)\n",
    "print(\"accuracy:\", acc)\n",
    "\n",
    "pres = precision(pred, ytest)\n",
    "print(\"precision:\", pres)\n",
    "\n",
    "rec = recall(pred, ytest)\n",
    "print(\"recall:\", rec)\n",
    "\n",
    "print(\"\\nWith Gini Impurity: \")\n",
    "pred = np.array(predict(tree2, xtest))\n",
    "\n",
    "acc = accuracy(pred, ytest)\n",
    "print(\"accuracy:\", acc)\n",
    "\n",
    "pres = precision(pred, ytest)\n",
    "print(\"precision:\", pres)\n",
    "\n",
    "rec = recall(pred, ytest)\n",
    "print(\"recall:\", rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19339e1b-db95-4565-8e34-801daacc961e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Information Gain: \n",
      "accuracy: 0.82\n",
      "precision: 0.8383838383838383\n",
      "recall: 0.8058252427184466\n",
      "\n",
      "With Gini Impurity: \n",
      "accuracy: 0.81\n",
      "precision: 0.7981651376146789\n",
      "recall: 0.8446601941747572\n"
     ]
    }
   ],
   "source": [
    "#tests results with scikit-learn library\n",
    "libraryTree = DecisionTreeClassifier(criterion=\"entropy\",min_samples_split=2)\n",
    "libraryTree.fit(xtrain, ytrain)\n",
    "libraryTree2 = DecisionTreeClassifier(criterion=\"gini\",min_samples_split=2)\n",
    "libraryTree2.fit(xtrain, ytrain)\n",
    "\n",
    "print(\"With Information Gain: \")\n",
    "pred = libraryTree.predict(xtest)\n",
    "\n",
    "acc = accuracy(pred, ytest)\n",
    "print(\"accuracy:\", acc)\n",
    "\n",
    "pres = precision(pred, ytest)\n",
    "print(\"precision:\", pres)\n",
    "\n",
    "rec = recall(pred, ytest)\n",
    "print(\"recall:\", rec)\n",
    "\n",
    "print(\"\\nWith Gini Impurity: \")\n",
    "pred = libraryTree2.predict(xtest)\n",
    "\n",
    "acc = accuracy(pred, ytest)\n",
    "print(\"accuracy:\", acc)\n",
    "\n",
    "pres = precision(pred, ytest)\n",
    "print(\"precision:\", pres)\n",
    "\n",
    "rec = recall(pred, ytest)\n",
    "print(\"recall:\", rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1490fe1-0f37-4efb-9dfd-58ae654b1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When interviewer asks what a decision tree is:\n",
    "# A decision tree is a machine learning algorithm that makes predictions by taking certain decisions. \n",
    "# And these decisions are selected because they can maximize some sort of criteria, like information gain for example,\n",
    "# which can hopefully efficently lead us to a correct interpretation of the data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
